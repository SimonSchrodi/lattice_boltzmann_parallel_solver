\documentclass[a4paper,11pt, footsepline]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
\usepackage{lmodern}
\usepackage{textcomp}
\usepackage{subfigure}
\usepackage{pgf}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[english]{babel}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{nameref}
\usepackage{hyperref}
\usepackage[linesnumbered, boxed]{algorithm2e}
\usepackage[automark,footsepline,plainfootsepline,headsepline]{scrpage2}
\usepackage{listings, newtxtt}
\lstset{basicstyle=\ttfamily, keywordstyle=\bfseries}

\SetAlCapSkip{1em}

\newcommand{\rulesep}{\unskip\ \vrule\ }


\renewcommand*{\algorithmcfname}{Code listing}
\renewcommand*{\algorithmautorefname}{Code listing}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\SetKwProg{Fn}{def}{:}{}
%\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
%\SetCommentSty{mycommfont}
\SetStartEndCondition{ }{}{}%
\SetKwComment{Comment}{$\triangleright$\ }{}
\SetKwFunction{Range}{range}%%
\SetKw{KwTo}{in}
\SetKwFor{For}{for}{\string:}{}%
%\renewcommand{\forcond}{$i$ \KwTo\Range{$n$}}
\AlgoDontDisplayBlockMarkers\SetAlgoNoEnd\SetAlgoNoLine%

\usepackage[printonlyused]{acronym}

\newcommand*{\fullref}[1]{\hyperref[{#1}]{\autoref*{#1} \nameref*{#1}}} % One single link
\renewcommand*\footnoterule{}

\makeatletter
\renewcommand{\@chapapp}{}% Not necessary...
\newenvironment{chapquote}[2][2em]
  {\setlength{\@tempdima}{#1}%
   \def\chapquote@author{#2}%
   \parshape 1 \@tempdima \dimexpr\textwidth-2\@tempdima\relax%
   \itshape}
  {\par\normalfont\hfill--\ \chapquote@author\hspace*{\@tempdima}\par\bigskip}
\makeatother



% Book's title and subtitle
\title{\Huge \textbf{High Performance Computing with Python} \vspace{4mm} \\ \huge Final Report}
% Author
% \author{\textsc{First-name Last-name}\footnote{email address}}
\author{\textsc{Name} \\ \vspace{3mm}\text{matricular number}  \\
\vspace{3mm}\text{mail}}


\begin{document}

\makeatletter
    \begin{titlepage}
        \begin{center}
            \includegraphics[width=0.5\linewidth]{logos/Uni_Logo-Grundversion_E1_A4_CMYK.eps}\\[4ex]
            {\huge \bfseries  \@title }\\[2ex] 
            {\LARGE  \@author}\\[30ex] 
            {\large \@date}
        \end{center}
    \end{titlepage}
\makeatother
\thispagestyle{empty}
\newpage



\tableofcontents

\begin{acronym}
\section*{Abbreviations}
\acro{bte}[BTE]{Boltzmann Transport Equation}
\acro{ci}[CI]{Continuous Integration}
\acro{lbm}[LBM]{Lattice Boltzmann Method}
\acro{simd}[SIMD]{Single Instruction Multiple Data}
\acro{mpi}[MPI]{Message Passing Interface}
\end{acronym}
\mainmatter

\chapter{Introduction}
The \acf{lbm} is a numerical, parallelizable and efficient scheme for simulating fluid flows based on the discretization of (continuous) \acf{bte}.\cite{McNamara.1988} In addition, the \ac{lbm} can be extended with boundary conditions. The key property of the \ac{lbm} is that it is a discrete kinetic theory approach featuring a mescoscale description of the microstructure of the fluid instead of discretizing macroscopic continuum equations. Other key advantages of the \ac{lbm} include: efficient implementation by parallelization and the \ac{lbm} can be applied to different kind of lattices.

We show in several two-dimensional (i.e. planar) test cases, i.e. \textit{Couette flow}, \textit{Poiseuille flow} and \textit{Von K\'{a}rm\'{a}n's vortex street}, the correctness of our implementation as well as the significant reduction of computational time of the von K\'{a}rm\'{a}n's vortex street simulation by means of parallelization by spatial domain decomposition using \textit{Python} as programming language with its highly efficient \textit{numpy} library \cite{Oliphant.2006,vanderWalt.2011} and the \acf{mpi} \cite{Dalcin.2005, Dalcin.2008, Dalcin.2011}.

All code is available at \url{https://github.com/infomon/lattice_boltzmann_parallel_solver} under BSD license. We give the instructions how to reproduce the results of the experiments conducted in this report in the \textit{README}.

\section*{Structure of report}
The remainder of the report is organized as follows:
\begin{itemize}
\item \textbf{Chapter \ref{ch-method}} describes the \ac{lbm}. More specifically, we describe how we discretize the \textit{\acf{bte}} resulting in the \ac{lbm}. We also show how macroscopic quantities, e.g. density and velocity, can be calculated from the microscopic simulation. In addition, we describe several boundary conditions that can be applied in the \ac{lbm}.
\item \textbf{Chapter \ref{ch-implementation}} describes how the \ac{lbm} is implemented using \textit{Python} as programming language. We also show how we parallelized the implementation and how we ensured software quality by unit testing.
\item \textbf{Chapter \ref{ch-results}} conducts extensive experiments showing the applicability and correctness of the implementation of the solver for the \ac{lbm}. 
\item \textbf{Chapter \ref{ch-conclusion}} concludes this report.
\end{itemize}
\chapter{Lattice Boltzmann Method}\label{ch-method}
\section{Overview}
In this chapter we describe the \acf{lbm}. The main idea of the \ac{lbm} is to \textit{simulate a fluid density statistically on a lattice} instead of solving (and also discretizing) the Navier-Stokes equations.
\section{Boltzmann Transport Equation}\label{sec-bte}
The \acf{bte} $\frac{df}{dt}$ defines the fundamental differential equation of kinematic gas theory. It describes the evolution of the probability density function $f_(\mathbf{r},\mathbf{v},t)$ for finding a molecule with mass $m$ and velocity $\mathbf{v}$ at position $\mathbf{r}$ over time $t$. Huang \cite{Huang.1987} shows that the \ac{bte} relaxes to the Maxwell velociy distribution function. Bhatnagar et al. \cite{Bhatnagar.1954} approximate the relaxation of $f$ towards $f^{eq}$ as follows:
\begin{equation}
\label{eq-bte}
\frac{df(\mathbf{r},\mathbf{v},t)}{dt}=-\frac{f(\mathbf{r},\mathbf{v},t)-f^{eq}(\mathbf{v};\rho(\mathbf{r,t}),\mathbf{u}(\mathbf{r},t),T(\mathbf{x},t))}{\tau}
\end{equation}
where $\tau$ is the so-called characteristic time, $\rho$ is the mass density, $u$ is the average velocity at position $\mathbf{x}$ and $T$ is the temperature (see section \ref{sec-momentUpdate} for more details). The characteristic time determines how fast the fluid converges towards the equilibrium depending on the viscosity of the fluid. The higher the viscosity, the slower it converges towards the equilibrium. Note, that eq. \ref{eq-bte} satisfies the Navier-Stokes equations.
\subsection*{Discretization of the \ac{bte}}
The \ac{bte} of eq. \ref{eq-bte} is defined in the continuous domain. In order to work with the \ac{bte} on the computer we have to discretize it in space, velocity and time. The space discretization can be done by just using a discrete lattice (e.g. two-dimensional array). To discretize the velocity and time we have to impose that the velocity multiplied with the time is equal to some integer, i.e. the particle can only travel on the given lattice and not in-between lattice nodes. 

We discretize the velocity directions with the D2Q9 scheme (see fig. \ref{fig:mesh}), which is two-dimensional and consists of nine discrete velocity directions. The velocity directions point to each of its neighbors in the Moore neighborhood. Note, that at the central lattice node the particle is at rest. We define the velocity vectors as follows:
\begin{equation}
\mathbf{c_i}=\begin{pmatrix}
0 & 1 & 0 & -1 & 0 & 1 & -1 & -1 & 1\\
0 & 0 & 1 & 0 & -1 & 1 & 1 & -1 & -1
\end{pmatrix}.
\end{equation}
Therefore, we discretize the probability density function $f(\mathbf{r},\mathbf{v},t)$ to obtain the discrete probability density function $f_{i}(\mathbf{x},t)$, where the subscript $i$ indicates the direction and $\mathbf{x}$ is the discrete lattice.
\begin{figure}
  \begin{center}
   \includegraphics[width=.3\textwidth]{../figures/lbm/d2q9.png}
   \caption{Discretization of the velocity space into nine discrete directions (D2Q9). The numbers $0,...,9$ uniquely identify the direction.}
  \label{fig:mesh}
  \end{center}
\end{figure}

Finally, we get the discretized version of eq. \ref{eq-bte}:
\begin{equation}
\label{eq-discretizedBTE}
\underbrace{f_{i}(\mathbf{x}+\mathbf{c}_{i}\Delta t, t+\Delta t) - f_{i}(\mathbf{x})}_{streaming}
=\underbrace{-\omega(f_{i}(\mathbf{x})-f_{i}^{eq}(\mathbf{x},t))}_{collision},
\end{equation}
where the \textit{streaming} and \textit{collision process} are the key steps in the \ac{lbm} and $\omega=\frac{\Delta t}{\tau}$ is a relaxation parameter.

The equilibrium probability density function $f_{i}^{eq}$ can be computed as follows:
\begin{equation}
\label{eq-equilibrium}
f_{i}^{eq}=w_{i}\rho(x,t)(1+3\mathbf{c}_{i}\mathbf{u}(\mathbf{x},t)+\frac{9}{2}(\mathbf{c}_{i}\mathbf{u}(\mathbf{x},t))^{2}-\frac{3}{2}\mathbf{u}^{2}(\mathbf{x},t)),
\end{equation}
where $w_{i}=
\begin{cases}
\frac{4}{9}, ~if~i=0\\
\frac{1}{9}, ~if~i=1,2,3,4\\
\frac{1}{36}, ~if~i=5,6,7,8
\end{cases}$. Fig. \ref{fig:streamingCollisionVis} gives an example for eq. \ref{eq-discretizedBTE} for a single node. In the streaming step, the node receives direction-specific probability density function values $f_{i}(\mathbf{x},t)$ from its nine neighbors. In the collision step, we relax the new probability density function values $f_{i}(\mathbf{x},t\Delta t)$ towards the equilibrium probability density function $f^{eq}_{i}$ and thereby take into account collisions between particles.
\begin{figure}
  \begin{center}
   \includegraphics[width=\textwidth]{../figures/lbm/vis.png}
   \caption{Visualization of the streaming and collision step. The arrows' length represent the normalized values of the distribution function for every discrete direction. Figure from \cite{Boix.2013}.}
  \label{fig:streamingCollisionVis}
  \end{center}
\end{figure}
\section{Moment update}\label{sec-momentUpdate}
In the \ac{lbm}, the density $\rho$ and velocity $\mathbf{u}$ are defined by the zeroth and first moments of the probability distribution function $f$, respectively:
\begin{align} 
\rho(\mathbf{x},t) &=  \int f(\mathbf{x}, \mathbf{u},t)~d^{3}\mathbf{u}, \\ 
\mathbf{u}(\mathbf{x},t) &=  \frac{1}{\rho(\mathbf{x})}\int f(\mathbf{x}, \mathbf{u},t)\cdot\mathbf{c}(\mathbf{u})~d^{3}\mathbf{u}.
\end{align}
The discretization of those equations yields
\begin{align}
\label{eq-density}
\rho(\mathbf{x}) &=  \sum\limits_{i} f_{i}, \\ 
\label{eq-velocity}
\mathbf{u}(\mathbf{x}) &=  \sum\limits_{i} f_{i}\mathbf{c}_{i}.
\end{align}
\section{Boundary conditions}\label{sec-boundaryConditions}
The boundary condition describes how the fluid flow behaves during streaming at the boundaries. We define the boundary node $\mathbf{x}_{b}$ to have at least one link to a solid or fluid node. 
Note, that the boundary conditions have to be placed in the correct step inside the \ac{lbm} (see code listing \ref{algo-pseudeocode}). For this reason we differentiate between the \textit{pre-streaming} probability density function $f_{i}^{\ast}$ and the \textit{post-streaming} probability density function $f_{i}$.
To apply boundary conditions the probability density function after the streaming $f_i$ is modified at each boundary node $\mathbf{x}_{b}$ given the pre-streaming probability density function $f_{i}^{\ast}$ in each time step:
\begin{equation}
f_i(\mathbf{x}_{b}+c_{i}\Delta t, t+\Delta t)=f_{i}^{\ast}(\mathbf{x}_b,t).
\end{equation}

One question that arises is where the boundary nodes $\mathbf{x}_b$ are defined. We distinguish between so called \textit{wet nodes} and \textit{dry nodes} due to different domains, i.e. computational and physical domain. In the former, the computation and physical domain is the same (i.e. the boundaries are placed on the lattice nodes) but this comes with a increased difficulty for the implementation. In the latter the physical domain is half a cell away from the computational domain (i.e. the boundaries are located between the lattice nodes) retaining second order accuracy as long as the boundary is placed exactly in the middle of the lattice nodes.

Below we describe several boundary conditions. One key advantage of the \ac{lbm} is its easy implementation of boundary conditions and in particular the arbitrary combination of boundary conditions as long as they do not contradict themselves.
\subsection*{Periodic boundary conditions}
For a periodic boundary condition the flow leaving a boundary re-enters the domain on the opposite side of the domain
\begin{equation}
f_{i}(\mathbf{x}_{1},t)=f_{i}(\mathbf{x}_{N},t),
\end{equation}
where $\mathbf{x}_{1}$ and $\mathbf{x}_{N}$ are the first and last node in the physical domain, respectively. Visually, we can imagine the bounce-back boundary conditions as if we have a cylindrical shape. Note, that therefore periodic boundary conditions conserve mass and momentum.
The periodic boundary condition is implicitly implemented by the streaming function.
\subsection*{Periodic boundary conditions with pressure variation}
The periodic boundary conditions with pressure variation add a density drop $\Delta\rho$ (or pressure drop $\Delta p$) between inlet and outlet. Note, that the pressure and density are related through the ideal gas of state $p=c_{s}^2\rho$, where $c_{s}$ is the speed of sound. \cite{CLAPEYRON.}
Let's assume that we want to model a pressure drop in x-direction, then it holds $\forall y\in\lbrace 1,...,l_{y}\rbrace$ that $p(x_{1},y,t)=p(x_{N},y,t)+\Delta\rho$, where $l_{y}$ denotes the diameter in y-direction and $x_1$ and $x_{N}$ denote the left-most and right-most node in the \ac{lbm}, respectively. Thus, we get $\rho_{out}=\frac{p_{out}}{c_{s}^2}$ and $\rho_{in}=\frac{p_{out}+\Delta p}{c_{s}^2}$, where the subscripts $in$ and $out$ denote the pressure values at the periodic boundaries.
Note, that the velocity is the same at the periodic boundaries: $\mathbf{u}(x_{1},y,t)=\mathbf{u}(x_{N},y,t)$.

Let's now assume virtual nodes $\mathbf{x}_{0}$ and $\mathbf{x}_{N+1}$ at both ends of the periodic boundaries. Note, that the virtual nodes $\mathbf{x}_{0}$ and $\mathbf{x}_{N}$ correspond to $\mathbf{x}_{N}$ and $\mathbf{x}_{1}$, respectively. Visually we can imagine this like (infinitely) many pipes connected to each other. We decompose the probability density function into a equilibrium part $f_{i}^{eq}$ and non-equilibrium part $f_{i}^{neq}$. The non-equilibrium probability density function is computed by $f_{i}^{neq}=f_{i}-f_{i}^{eq}$. Combining the correspondences of virtual nodes and nodes in the physical domain as well as the decomposition into (non)-equilibrium probability density function parts we obtain the inlet and outlet boundary condition, respectively:
\begin{align}
f_{i}^{\ast}(x_0,y,t)&=f_{i}^{eq}(\rho_{in},\mathbf{u_N})+\underbrace{(f_{i}^{\ast}(x_N,y,t)-f_{i}^{eq}(x_N,y,t)}_{f_i^{neq}(x_N,y,t)},~and\\
f_{i}^{\ast}(x_{N+1},y,t)&=f_{i}^{eq}(\rho_{out},\mathbf{u_1})+\underbrace{(f_{i}^{\ast}(x_1,y,t)-f_{i}^{eq}(x_1,y,t)}_{f_i^{neq}(x_1,y,t)}.
\end{align}
\subsection*{Bounce-back boundary}
The bounce-back boundary condition applies a no-slip condition at the boundary. It simulates the interaction between the fluid with a non-moving wall without slip. It can also be applied to a stationary obstacle such as a plate.
\begin{equation}
f_{\bar{i}}(\mathbf{x}_{b},t+\Delta t)=f_{i}^{\ast}(\mathbf{x}_{b},t),
\end{equation}
where the index $\bar{i}$ denotes the conjugate channel of $i$, e.g. the conjugate channel of $1$ is equal to $3$.
\subsection*{Moving wall}
The moving wall extends the bounce-back boundary condition by taking into account the gain or lose of momentum of particles during interaction with the moving wall. Thus, we extend the bounce-back boundary condition with an extra term for the momentum change
\begin{equation}
f_{\bar{i}}(\mathbf{x}_{b},t+\Delta t)=f_{i}^{\ast}(\mathbf{x}_{b},t)-2\omega_{i}\rho_{w}\frac{\mathbf{c}_{i}\cdot\mathbf{u}_{w}}{c_{s}^{2}},
\end{equation}
where $c_{s}$ is the speed of sound, $\rho_{w}$ and $\mathbf{u}_{w}$ are the density and velocity at the wall, respectively. The velocity at the wall $\mathbf{u}_{w}$ is equal to $\begin{pmatrix}U_{w}\\0\end{pmatrix}$ for a tangentially moving wall in x-direction with wall velocity $U_{w}$.  
There are two main options for the estimation of the density at the wall $\rho_{w}$:
\begin{enumerate}
\item The density at the wall $\rho_{w}$ is equal to the \textit{average} density $\bar{\rho}$.
\item The density at the wall $\rho_{w}$ is \textit{extrapolated} from the densities $\rho$ next to the wall. Depending on the order of the extrapolation, we use more or less nodes.
\end{enumerate}
\subsection*{Open boundary}
\cite{Kruger.2016} describe open boundaries consist of inlets and outlets where the flow can either enter or leave the computation domain and where we typically \textit{impose velocity or density profiles}. We implement the inlet as follows:
\begin{equation}
f_{i}(\mathbf{x}_{b},t+\Delta t)=f_{i}^{eq}(\rho_{in},\mathbf{u}_{in})~\forall i\in\lbrace 0,...,8\rbrace,
\end{equation}
where $\rho_{in}$ and $\mathbf{u}_{in}$ are the density and velocity at the inlet, respectively.

For the outlet, we implement a first-order extrapolation scheme by using the information from the second last node $\mathbf{x}_{b_{2}}=\mathbf{x}_{b}-\Delta\mathbf{x}$
\begin{equation}
f_{i}(\mathbf{x}_{b},t+\Delta t)=f_{i}(\mathbf{x}_{b_{2}},t),
\end{equation}
where $i$ denotes the indices pointing into the domain.
\chapter{Implementation}\label{ch-implementation}
In this chapter we will describe how we implement the algorithm using \textit{Python} as programming language.
\section{Overview}
Code listing \ref{algo-pseudeocode} shows the pseudocode of the iteration loop of the \ac{lbm}. As input we can specify the geometry of the physical domain, the boundary conditions (see section \ref{sec-boundaryConditions} for more details) as well as the initial conditions. 

First we initialize the density $\rho$ and velocity $\mathbf{u}$ and compute the initial value of the probability density function $f_{i}^{eq}=f_{i}$. 

Then we iterate in a loop over several steps as long as the stopping criterion (e.g. maximum time steps) is not satisfied. Note, that there is some flexibility when to apply which step. \cite{Kruger.2016, Succi.2018} The following order of steps corresponds to the order in the implementation of the \ac{lbm}. We first compute the equilibrium function $f_{i}^{eq}$ given the current density $\rho$ and velocity $\mathbf{u}$. In the collision step we simulate the effects of collisions between particles (see section \ref{sec-bte} for more details). After that, we simulate the streaming of $f_i$, i.e. we simulate the movement of particles to the nearest neighbour lattice nodes using the D2Q9 discretization. Then we apply potential boundary conditions on the probability density function $f_i$. Note, that we first apply the streaming operation at every node (including the boundary nodes $\mathbf{x}_{b}$) and then correct the boundary nodes $\mathbf{x}_{f}$ after the streaming. This has the advantage that the implementation of the streaming is easier. Lastly, we compute the density $\rho$ and velocity $\mathbf{u}$ (see section \ref{sec-momentUpdate} for details on the formulas on how to compute the macroscopic quantities).

After running the \ac{lbm} we can obtain the density $\rho$ and velocity $\mathbf{u}$ as macroscopic quantities.

\begin{algorithm}
 \caption{\label{algo-pseudeocode}Pseudocode of the iteration loop of the \ac{lbm}.}
     \SetAlgoLined
     \KwInput{Geometry and parameters $l$, $h$, $U$, $\nu$,...; boundary conditions; initial conditions}
     \KwOutput{Final density $\rho$ and velocity $\mathbf{u}$}
     initialize $\rho$ and $\mathbf{u}$ \\
     compute $f_i$ and $f_i^{eq}$ \\
     \While{stopping criterion is not satisfied}{
      compute equilibrium function $\rho,~\mathbf{u}\rightarrow f_{i}^{eq}$\Comment*[f]{eq. \ref{eq-equilibrium}}\\
      collision step $f_{i}^{\ast}=f_{i}(\mathbf{x},t)-\frac{\Delta t}{\tau}(f_{i}(\mathbf{x},t)-f_{i}^{eq}(\mathbf{x},t))$ \Comment*[f]{eq. \ref{eq-discretizedBTE}}\\
      streaming $f_{i}(\mathbf{x}+c_{i}\Delta t, t+\Delta t)=f_{i}^{\ast}(\mathbf{x},t)$ \Comment*[f]{eq. \ref{eq-discretizedBTE}}\\
      apply boundary conditions $f_i(\mathbf{x}_{b}+c_{i}\Delta t, t+\Delta t)=f_{i}^{\ast}(\mathbf{x}_b,t)$ \Comment*[f]{section \ref{sec-boundaryConditions}}\\
      moment update $f_i \rightarrow \rho,~\mathbf{u}$ \Comment*[f]{eq. \ref{eq-density} \& \ref{eq-velocity}}
     }
\end{algorithm}
\section{Basic implementation in Python}
In this section we show how we implemented the basic equations and data structures introduced in chapter \ref{ch-method}. We use Python as programming language and use the python libraries \textit{numpy} \cite{Oliphant.2006,vanderWalt.2011} for array operations, \textit{scipy} \cite{Virtanen.2020} for some more complex scientific computations and \textit{matplotlib} \cite{Hunter.2007} for visualizing the obtained results. And key advantage of numpy is to vectorize arrays, which lowers the computational time.

We represent the (discrete) probability density function $f_{i}(\mathbf{x})$ as a numpy array of size $l_{x}\times l_{y}\times 9$, where $l_{x}$ and $l_{y}$ are the size of the lattice in x- and y-direction, respectively. The last dimension (e.g. $9$) of the numpy array corresponds to the discretized velocity direction.
We represent the velocity directions $\mathbf{c}$ and the weights $w_{i}$ as numpy arrays with size $9\times 2$ and $9$, respectively.

In steps 4 and 5 of code listing \ref{algo-pseudeocode} we simulate the collision part of eq. \ref{eq-discretizedBTE}. We implement the computation of the equilibrium probability density function $f_{i}^{eq}$ of eq. \ref{eq-equilibrium} and the right-hand side of eq. \ref{eq-discretizedBTE} with vectorized numpy code. 

Code listing \ref{algo-collision} shows the implementation of the collision step, separated into equilibrium probability density function computation and the collision.
\begin{algorithm}
 \caption{\label{algo-collision}Python implementation of the collision step. Note that the symbol $@$ is a shorthand for the matrix multiplication in NumPy.}
     \SetAlgoLined
     \KwInput{density $\rho$; velocity $\mathbf{u}$, relaxation parameter omega $\omega$}
     \KwOutput{Probability density function before streaming $f_{i}^{\ast}$}
     w\textunderscore i = np.array([4/9, 1/9, 1/9, 1/9, 1/9, 1/36, 1/36, 1/36, 1/36])\\
     \SetKwFunction{FEq}{f\textunderscore eq}
\Fn{\FEq{rho:np.ndarray, u:np.ndarray}$\rightarrow$np.ndarray}{
w\textunderscore i = w\textunderscore i[np.newaxis, np.newaxis, ...]\\
		ci\textunderscore u = u @ c\textunderscore i.T\\
		uu = (np.linalg.norm(u, axis=-1) ** 2)[..., np.newaxis]\\
		rho = rho[..., np.newaxis]\\
        \KwRet w\textunderscore i * rho * 
        (1 +
            3 * ci\textunderscore u +
            9/2 * ci\textunderscore u ** 2 -
            3/2 * uu)
  }
  f\textunderscore pre = f + (f\textunderscore eq(rho, u) - f) * omega
\end{algorithm}

In steps 6 of code listing \ref{algo-pseudeocode} we simulate the streaming part of eq. \ref{eq-discretizedBTE}. Code listing \ref{algo-streaming} shows the implementation using \textit{np.roll}.  The function rolls the the data in the direction specified by the \textit{axis} argument. With the argument $shift$ we specify the number of places elements are shifted according to the discrete velocity directions $\mathbf{c}$. Note, that the function automatically implements the periodic boundary condition.
\begin{algorithm}
 \caption{\label{algo-streaming}Python implementation of the streaming step.}
     \SetAlgoLined
     \KwInput{Probability density function before streaming $f_{i}^{\ast}$}
     \KwOutput{Probability density function after streaming $f_{i}$}
     c\textunderscore i = np.array([[0,0],[1,0],[0,1],[-1,0],[0,-1],[1,1],[-1,1],[-1,-1],[1,-1]])\\
     \SetKwFunction{FStream}{streaming}
\Fn{\FStream{f\textunderscore pre:np.ndarray}$\rightarrow$np.ndarray}{
f\textunderscore post = np.zeros\textunderscore like(f\textunderscore pre)\\
\For{$i$ \KwTo\Range $(9)$}{
f\textunderscore post[..., i]=np.roll(f\textunderscore pre[...,i], shift=c\textunderscore i[i], axis=(0,1))
}
\KwRet f\textunderscore post
}
\end{algorithm}

For the other boundary conditions we implement additional functions which correct the boundary nodes as described in section \ref{sec-boundaryConditions}. Code listing \ref{algo-boundaryConditions} gives an overview of the different implementations.
The implementation of the boundary conditions follows directly from the formulas. For example when we implement the bounce-back boundary condition on a non-moving wall at the bottom, we bounce back channels $4$, $7$ and $8$. That is, we assign the pre-streaming probability density function at the boundary nodes $f_{i}^{\ast}(\mathbf{x}_{b},t)$ to the conjugate channels (e.g. $2$, $5$, $6$) of the post-streaming probability density function at the boundary nodes $f_{i}(\mathbf{x}_{b},t+\Delta t)$. 
For the implementation of the bounce-back condition for a object inside the domain such as a vertical plate we apply the bounce-back conditions on the corresponding two columns. We have to take corner nodes into special consideration. There we just bounce-back two channels. For example consider the lattice node at the top corner on the left side of the plate. There we only bounce back channels $1$ and $8$.
We implement the moving wall boundary condition in a similar way but with the additional term $-2\omega_{i}\rho_{w}\frac{\mathbf{c}_{i}\cdot \mathbf{u}_{w}}{c_{s}^{2}}$ to take the momentum change into account. For the density at the wall $\rho_{w}$ we use the average density.
For the inlet boundary condition we assign the equilibrium probability density function given the inlet density $\rho_{in}$ and inlet velocity $\mathbf{u}_{in}$. 
For the outlet boundary condition we assign the second last nodes of the probability density function of the previous time step to the last nodes of the probability density function of the current time step.
For the periodic boundary conditions with pressure variation we extend the domain with virtual nodes at both ends of the periodic boundaries. The implementation then is straightforward. 
\begin{algorithm}
 \caption{\label{algo-boundaryConditions}Exemplar Python implementation of the boundary condition. Note that the index i refers to specific channels depending on the boundary conditions. For details see section \ref{sec-boundaryConditions}. Note that $@$ and $.T$ are shorthands for matrix multiplication and transpose in NumPy.}
     \SetAlgoLined
     \# Bounce-back\\
     f\textunderscore post[$\mathbf{x}$\textunderscore b, conjugate[i]]=f\textunderscore pre[$\mathbf{x}$\textunderscore b,i] \\
     \# Moving wall\\
          f\textunderscore post[$\mathbf{x}$\textunderscore b, conjugate[i]]=f\textunderscore pre[$\mathbf{x}$\textunderscore b,i] - 2$\cdot$ w\textunderscore i[i]$\cdot$ avg\textunderscore rho $\cdot$ (c\textunderscore i[i] @ u\textunderscore w) /(c\textunderscore s **2) \\
          \# Inlet\\
     f\textunderscore post[0,:, i]=f\textunderscore eq(rho\textunderscore in,u\textunderscore in)[0,:,i]\\
     \# Outlet\\
     f\textunderscore post[-1,:, i]=f\textunderscore previous[-2,:,i]\\
     \# Periodic boundary condition with pressure variation\\
  f\textunderscore pre[0,:,i]=f\textunderscore eq(rho\textunderscore in, u[-2,...])[...,i].T + (f\textunderscore pre[-2,:,i]-f\textunderscore eq(rho, u)[-2,:,i])
\end{algorithm}
\section{Parallelization}\label{sec-parallelization}
We parallelize the \ac{lbm} using spatial domain decomposition and \acf{mpi} \cite{Dalcin.2005, Dalcin.2008, Dalcin.2011}. \ac{mpi} is a \textit{communication protocol} for programming parallel computers, i.e. \ac{simd} \cite{Flynn.1972} by executing the same operation on multiple data points simultaneously.

We decompose the computational domain into sub domain using a cartesian topology. To implement this we use the \ac{mpi} function \textsf{Create\textunderscore cart} creating a cartesian topology.
We decompose the full computational domain into sub domains of roughly equal size (see fig. \ref{fig:parallelizationScheme} for an example).
If the domain is not exactly divisible into the subdomains we extend the rightmost or topmost process with the division remainder. Note that this could lead to imbalanced subdomains, but comes with an easier transformation from global coordinates into local (i.e. process-level) coordinates. For the communication we extend the sub domains with ghost cells around the actual computational domain.

The collision step of the \ac{lbm} is embarrassingly parallel, since the collision step operates only locally and thus requires no communication between processes.

For the streaming step we have to consider particles moving from one domain to a neighboring domain (i.e. another process) corresponding the the channel. We implement this by extending each domain by \textit{ghost cells} around the actual computational domain. Before streaming we communicate the lattice nodes adjacent to the ghost region into the ghost points of the neighboring domain according to the specific channel. 

\begin{figure}
  \begin{center}
  \subfigure[Communication in x direction.]{\includegraphics[width=0.4\textwidth]{../figures/parallelization_strategy/parallelization_scheme_x.png}}
\rulesep
    \subfigure[Communication in y direction.]{\includegraphics[width=0.5\textwidth]{../figures/parallelization_strategy/parallelization_scheme_y.png}}
   \caption{Example of spatial domain decomposition and communication strategy. We decompose the original domain into a $2\times 2$ grid of sub domains of roughly equal size (blue lattice points). We add additional ghost nodes around the actual computational domain (white lattice points). In the communication step, we communicate the rightmost, leftmost, bottommost and topmost lattice points in the actual computational domain into the neighboring ghost lattice points. In the left subfigure (a) we show the communication step in the right direction, i.e. respective  right neighbors except for the edge domains. In the right subfigure (b) we show the communication step in the top direction.}
  \label{fig:parallelizationScheme}
  \end{center}
\end{figure}
Let's consider the example of fig. \ref{fig:parallelizationScheme}. We consider a $2\times 2$ decomposition of the original computational domain and we denote the lower left subdomain with rank $0$, upper left with rank $1$, lower right with rank $2$ and upper right with rank $3$. W.l.o.g., we first consider the rightmost nodes $\mathbf{x}_r$ of each sub domain. We communicate the rightmost nodes $\mathbf{x}_r$ from each process to the neighboring process on the right. In our particular example this means that we communicate the rightmost nodes $\mathbf{x}_r$ of the process with rank $0$ to the ghost cells on the left side of process with rank $1$. Accordingly, we communicate from process $2$ to process $3$, $2$ to $1$ and $3$ to $1$. Note that we do communicate from process $2$ to $0$ and $3$ to $1$ or more generally we do communicate outer boundaries of edge domains, since we set \textsf{periods=(True, True)} when calling the \ac{mpi} function \textsf{Create\textunderscore cart}.\footnote{Note, that we could set \textsf{periods=(False, True)} for the von K\'{a}rm\'{a}n's vortex street experiment, since we set the inlet and outlet boundary nodes in every time step. However, for sake of generality we set \textsf{periods=(True, True)}.} This implicitly implements the periodic boundary conditions.
We repeat this procedure for the leftmost nodes $\mathbf{x}_l$, bottommost nodes $\mathbf{x}_b$ and topmost nodes $\mathbf{x}_t$ (see right subfigure of fig. \ref{fig:parallelizationScheme}) accordingly. This totals in four communication steps. Code listing \ref{algo-communication} shows the implementation of the communication step. We call the communication step right before the streaming step in code listing \ref{algo-pseudeocode}.
\begin{algorithm}
 \caption{\label{algo-communication}Python implementation of the communication step.}
     \SetAlgoLined
     \SetKwFunction{OuterComm}{communication}
\textbf{from} mpi4py \textbf{import} MPI\\
\textbf{from} typing \textbf{import} Callable\\
\Fn{\OuterComm{comm: MPI.Intracomm}$\rightarrow$Callable[[np.ndarray], np.ndarray]}{
    left\textunderscore src, left\textunderscore dst = comm.Shift(direction=0, disp=-1)\\
    right\textunderscore src, right\textunderscore dst = comm.Shift(direction=0, disp=1)\\
    bottom\textunderscore src, bottom\textunderscore dst = comm.Shift(direction=1, disp=-1)\\
    top\textunderscore src, top\textunderscore dst = comm.Shift(direction=1, disp=1)\\
    
    	\SetKwFunction{InnerComm}{communicate}
	\Fn{\InnerComm{f: np.ndarray}$\rightarrow$np.ndarray}{
	        \# send to left\\
        recvbuf = f[-1, ...].copy()\\
        comm.Sendrecv(f[1, ...].copy(), left\textunderscore dst, recvbuf=recvbuf, source=left\textunderscore src)\\
        f[-1, ...] = recvbuf\\
        \# send to right\\
        recvbuf = f[0, ...].copy()\\
        comm.Sendrecv(f[-2, ...].copy(), right\textunderscore dst, recvbuf=recvbuf, source=right\textunderscore src)\\
        f[0, ...] = recvbuf\\
        \# send to bottom\\
        recvbuf = f[:, -1, :].copy()\\
        comm.Sendrecv(f[:, 1, :].copy(), bottom\textunderscore dst, recvbuf=recvbuf, source=bottom\textunderscore src)\\
        f[:, -1, :] = recvbuf\\
        \# send to top\\
        recvbuf = f[:, 0, :].copy()\\
        comm.Sendrecv(f[:, -2, :].copy(), top\textunderscore dst, recvbuf=recvbuf, source=top\textunderscore src)\\
        f[:, 0, :] = recvbuf\\
        \KwRet f

}
    \KwRet \InnerComm
    }

\end{algorithm}

In the boundary conditions we transform global coordinates $global\textunderscore coord$ into local (i.e. process-level) coordinates $local\textunderscore coord$ in order to apply the boundary conditions at the correct global position in the lattice grid. Since we only increase the size of sub domains on the right or top, we can compute local coordinates straightforward. From the \ac{mpi} function \textsf{Get\textunderscore coords} we get the process coordinates $proc\textunderscore coord$ of the cartesian topology. We compute the local $x$ coordinate $local\textunderscore coord_{x}$ as follows:
\begin{equation}
local\textunderscore coord_{x}=global\textunderscore coord_{x}-proc\textunderscore coord_{x}*(l_{x}//proc\textunderscore size_{x}) +1,
\end{equation}
where $l_{x}$ is the lattice grid size in $x$ direction and $//$ denotes the integer division. Note, that we add $1$, since we have to take the ghost cell in the specific process into account. We can apply computation similarly for the local $x$ coordinate $local\textunderscore coord_{y}$.
\section{Software quality}
\subsection*{Static typing}
Python is a dynamically typed language. That is that the Python interpreter does type checking only in runtime and the type of a variable is allowed to change. The opposite of dynamic typing is static typing. It is introduced by \textit{PEP 484}\footnote{\url{https://www.python.org/dev/peps/pep-0484/}} in Python. In static typing, the types of the variables are checked before runtime and the change of types is generally not allowed. Note, as an exception type casting is a way to change the type of a variable in many languages.

Dynamic typing allows for rapid prototyping and thus it enables fast software development. On the other side static typing can help to catch errors due to type errors, document the code and help to build a cleaner software architecture. The last point in particular ensures that the programmer thinks about the types of the variables and uses the correct types. Thus, in any larger project typing is critical to build and maintain clean code.
\subsection*{Unit testing}
One key component of every software project is extensive testing of the software. To this end, we implement several unit tests in order to validate the expected behavior of the implemented functions. 
More specifically, we test the computation of the density and velocity, the streaming function, mass preservation (i.e. first and second mass conservation equation and first and second impulse conservation equation from the Navier-Stokes Equations as well as a long run mass conservation test over $10000$ time steps) and the boundary conditions.
Additionally, we validate the parallelized implementation by comparing it to the serial implementation of the von K\'{a}rm\'{a}n's vortex street over $400$ time steps using different number of nodes. 

We integrate the unit tests into \ac{ci} using \textit{Travis CI} as build server so that the implementation and its potential unintentional modifications are validated for each commit to the repository.
\chapter{Numerical results}\label{ch-results}
To demonstrate the \ac{lbm} implementation we conduct several experiments with different combinations of boundary conditions. First we consider a \textit{shear wave decay} (section \ref{sec-shearWave}) to validate whether our implementation preserves mass as well as to show how $\omega$ relates to the kinematic viscosity $\nu$. In sections \ref{sec-Couette} and \ref{sec-pouseuille} we implement well known laminar flows, i.e. \textit{Couette} and \textit{pouiseuille} flow, from the literature and compare it to their analytical solutions, respectively. In section \ref{sec-karman} we implement the \textit{von K\'{a}rm\'{a}n's vortex street} and in the following section \ref{sec-scaling} how we can reduce computational complexity by spatial domain decomposition as introduced in section \ref{sec-parallelization}.
\section{Shear wave decay}\label{sec-shearWave}
The shear wave decay simulates a physical domain with only periodic boundary conditions which is in a initial state and its incremental steps towards the equilibrium state. One common practical and illustrative application of it, is the simulation of the breaking of a water dam. This exemplar application also shows the importance of simulations as the \ac{lbm} to simulate rather than applying it in a real-world setting potentially causing mass destruction and high costs.

We choose the following simulation parameters for our experiments:
\begin{itemize}
\setlength\itemsep{0.15em}
\item lattice grid shape $=50\times 50$
\item $\omega=1.0$
\item Sinusoidal density in x-direction $\rho(\mathbf{x},0)=\rho_{0}+\epsilon_{\rho}\sin(\frac{2\pi x}{l_x})$
\begin{itemize}
\setlength\itemsep{0.1em}
\item $\rho_{0}(\mathbf{x})=0.5$
\item $\epsilon_{\rho}=0.08$
\item $\mathbf{u}_{initial}(\mathbf{x})=0.0$
\end{itemize}
\item Sinusoidal velocity in y-direction $\mathbf{u}_{x}(\mathbf{x},0)=\epsilon_{\mathbf{u}}\sin(\frac{2\pi y}{l_y})$
\begin{itemize}
\setlength\itemsep{0.1em}
\item $\rho_{initial}(\mathbf{x})=0.0$
\item $\epsilon_{\mathbf{u}}=0.08$
\end{itemize}
\end{itemize}

In our experiments we only use periodic boundary conditions and we set the initial density and velocity as described in the above bullet list.

Fig. \ref{fig:evolution_density_surface} and \ref{fig:evolution_velocity_surface} show results for the two experiments. For the first experiment shown in the fig. \ref{fig:evolution_density_surface} we can imagine this as a wave in a swimming pool at an arbitrary. When we neglect environmental influences, such as wind, the wave will flatten out to an equilibrium state (without any wave). For sake of simplicity we neglect the periodic boundary condition here for the intuition. We can observe the same behavior in fig. \ref{fig:evolution_density_surface}. The wave has a smaller amplitude every time step until its reaches the equilibrium state. We can observe the same behavior in fig. \ref{fig:evolution_velocity_surface} when we have an initial sinusoidal velocity. The experiment validates that our \ac{lbm} implementation works correctly.
\begin{figure}
  \begin{center}
	\scalebox{0.7}{\input{../figures/shear_wave_decay/evolution_density_surface.pgf}}
   \caption{Evolution of the initial sinusoidal density over $2500$ time steps.}
  \label{fig:evolution_density_surface}
  \end{center}
\end{figure}
\begin{figure}
  \begin{center}
	\scalebox{0.7}{\input{../figures/shear_wave_decay/evolution_velocity_surface.pgf}}
   \caption{Evolution of the initial sinusoidal velocity over $2500$ time steps.}
  \label{fig:evolution_velocity_surface}
  \end{center}
\end{figure}

Fig. \ref{fig:omega_vs_viscosity} shows the effect on the viscosity $\nu$ when we vary the relaxation parameter $\omega$ for a given sinusoidal density or velocity. For the experiment we compute the simulated viscosity based on the exponential decay curve of the density and velocity using SciPy's function \textsf{curve\textunderscore fit}, respectively. Note, since the densities swing quite a lot and we thus do not have a smooth exponential decay curve we only take the maximums of the swinging into account. To obtain them we use SciPy's function \textsf{argrelextrema}. We can obtain the analytical solution for a given $\omega$ as follows
\begin{equation}
\nu = \frac{1}{3}(\frac{1}{\omega}-\frac{1}{2}).
\end{equation}
Note that we use the log scale on the y-axis for plotting. Thus, one has take the scaling into account, when arguing about the results.

\begin{figure}
  \begin{center}
	\scalebox{0.7}{\input{../figures/shear_wave_decay/meas_visc_vs_omega.pgf}}
   \caption{Viscosity $\nu$ over different relaxation parameters $\omega$ using $2500$ time steps for each run. In the left subfigure, we set a sinusoidal density as described above and set all velocities to zero as initial condition. In the right subfigure, we set a sinusoidal velocity given the parameters as described above and set all densities to zero. Note, that the y-axis is in log scale.}
  \label{fig:omega_vs_viscosity}
  \end{center}
\end{figure}
We can observe that the analytical viscosity matches the simulated viscosity quite well. However, when the relaxation parameter $\omega$ gets closer to $0$ or $2$ the error between analytical viscosity and simulated viscosity tends to increase. In other words, we can observe that our \ac{lbm} implementation gets numerically unstable when $\omega$ moves towards its minimum or maximum. We have to take this finding into consideration for our other experiments, because choosing an unsuited relaxation parameter $\omega$ can lead to wrong results just because of numerical instabilities.
\section{Planar Couette flow}\label{sec-Couette}
The planar Couette flow is a steady, laminar flow between two infinitely long, parallel plates with a fixed distance. One of those plates moves tangentially at a velocity of $U$ relative to the other plate, which itself is stationary. The flow is caused by the viscous drag force acting on the fluid. For the Couette flow exists an analytical solution, so that we can test the numerical simulation results of our implementation of the \ac{lbm}. The analytical solution is given by
\begin{equation}
\label{eq:couetteEq}
u(y)=-U\frac{y}{h},
\end{equation}
where $h$ is distance between the two plates and $u(y)$ is the velocity distribution for a given spatial coordinate normal to the plates $y$. For sake of clarity we use $h$ instead of $ly$ in eq. \ref{eq:couetteEq}. In the implementation those values are equal.

In our experiment, we apply the bounce-back boundary condition at the top wall, the moving wall at the bottom wall and periodic boundary conditions at the inlet and outlet. We choose the following simulation parameters for our experiments about the planar Couette flow:
\begin{itemize}
\setlength\itemsep{0.15em}
\item lattice grid shape $=20\times 30$
\item $\omega=1.0$
\item $U=0.05$
\item $\rho_{initial}(\mathbf{x})=1.0$
\item $\mathbf{u}_{initial}(\mathbf{x})=0.0$
\item time steps $=4000$
\end{itemize}

Fig. \ref{fig:Couette_vectors}(a) shows the visual results of the experiment. We can observe that the maximum velocity in x-direction is maximal at the bottom wall and minimal at the top wall. At the bottom wall the velocity in x-direction is exactly the same as the tangential velocity of $0.05\frac{lu}{s}$ of the moving wall. At the top wall the velocity in x-direction is equal to $0$. Other velocities at position $y$ can be linearly interpolated. When we linearly interpolate those points we get a slope of $m=-\frac{1}{600}$ and intercept of $c=0.05$. When comparing this to the analytical solution from eq. \ref{eq:couetteEq}, this is exactly the same. This results is verified by fig. \ref{fig:Couette_vectors}(b) which shows no absolute error (i.e. $0$). Note, that we set values less than $10^{-4}$ to $0$, since we attribute those tiny absolute errors to numerical instabilities.
\begin{figure}
  \begin{center}
  \subfigure[Visual result]{
	\scalebox{0.4}{\input{../figures/couette_flow/vel_vectors.pgf}}}
	  \subfigure[Absolute error]{
	\scalebox{0.4}{\input{../figures/couette_flow/absolute_error.pgf}}}
   \caption{Results of the simulation of the couette flow for our \ac{lbm} implementation after $4000$ time steps. In (a) we show the results of the velocity front to which the system is converged. In (b) we show the absolute error of the simulation results compared to the analytical solution.}
  \label{fig:Couette_vectors}
  \end{center}
\end{figure}

Fig. \ref{fig:Couette_evolution} shows the evolution of the Couette flow. As we can see, the moving wall drags nearby particles, which in turn drag neighboring particles (with less force). In the first time steps particles nearby the moving wall accelerate to the velocity of the moving wall. Those particles in turn accelerate adjacent particles. However, we can see that the acceleration of the other particles is less rapid the further away the particle from the moving wall. After around the $600$th time step the velocities in x-direction are almost exactly the same as the analytical solution given in eq. \ref{eq:couetteEq}. However, after those time steps there are minor improvements of the simulation results towards the true analytical solution.
\begin{figure}
  \begin{center}
	\scalebox{0.7}{\input{../figures/couette_flow/vel_vectors_evolution.pgf}}
   \caption{Couette flow evolution over $4000$ time steps. Note that the axis correspond to the same axis in fig. \ref{fig:Couette_vectors}(a). However, we have left them out here for the sake of clarity.}
  \label{fig:Couette_evolution}
  \end{center}
\end{figure}
\section{Planar Poiseuille flow}\label{sec-pouseuille}
The planar Poiseuille flow is a steady flow between two non-moving plates. The flow is caused by a constant pressure gradient $\frac{dp}{dx}$ in the axial direction, $x$, parallel to two infinitely long parallel plates, separated by a distance $h$.
As for the Couette flow, there exists an analytical solution for the Poiseuille flow, so that we can test the numerical simulation results of our implementation of the \ac{lbm}. The analytical solution is given by
\begin{equation}
\label{eq:poiseuilleEq}
u(y)=-\frac{1}{2\mu}\frac{dp}{dx}y(h-y),
\end{equation} 

In our experiments, we apply bounce-back boundary conditions at the bottom and top wall and the periodic boundary condition with pressure variation at the inlet and outlet nodes.
We choose the following simulation parameters for our experiments about the planar Poiseuille flow:
\begin{itemize}
\setlength\itemsep{0.15em}
\item lattice grid shape $=200\times 60$
\item $\omega=1.5$
\item $p_{out}=\frac{1}{3}$
\item $\Delta p=0.001$
\item $\rho_{initial}(\mathbf{x})=1.0$
\item $\mathbf{u}_{initial}(\mathbf{x})=0.0$
\end{itemize}

Fig. \ref{fig:Poiseuille_vectors}(a) shows the visual results of our experiment. We can observe that the velocity profile has a parabolic shape. The velocity profiles of channels $x=1$ and $x=100$  match the analytical solution described in eq. \ref{eq:poiseuilleEq} almost exactly. Also we can observe that the velocities in x-direction of both channels overlap. When comparing the area under both curves we get a difference of less than $0.02\%$. This shows that the flow behaves similarly at the inlet as well as in the middle. Its maximal velocity is in the middle (i.e. $y=100$) and the minimal velocities are at the top and bottom boundary. The maximal velocity is $0.01\frac{lu}{s}$ and the minimal velocities are $0\frac{lu}{s}$. The other velocities can be fitted by a quadratic function (i.e. eq. \ref{eq:poiseuilleEq}). When we compute the quadratic function with SciPy's function \textsf{curve\textunderscore fit}, we get $-4.48*10^{-5}y^{2}+2.64*10^{-3}y+1.34*10^{-3}$. This formula exactly matches the analytical solution described in eq. \ref{eq:poiseuilleEq}. This result is verified by fig. \ref{fig:Poiseuille_vectors}(b) which shows there is almost no absolute error between the simulated solution at $x=100$ and the analytical solution but on the boundaries. Note, that we clip values smaller than $10^{-4}$ to $0$, since we attribute those small absolute errors to numerical instabilities. The slight higher absolute errors at the boundaries are probably due to the some inaccuracies that are introduced by the bounce-back boundary condition. \cite{Kruger.2016}
\begin{figure}
  \begin{center}
    \subfigure[Visual result]{
	\scalebox{0.4}{\input{../figures/poiseuille_flow/vel_vectors.pgf}}}
	    \subfigure[Absolute error]{
	\scalebox{0.4}{\input{../figures/poiseuille_flow/absolute_error.pgf}}}
	\subfigure[Pressure along centerline]{
	\scalebox{0.4}{\input{../figures/poiseuille_flow/density_along_centerline.pgf}}}
   \caption{Poiseuille flow after $40000$ time steps. In the left subfigure (a) we show the Poiseuille flow at the first channel ($x=1$, cyan) and at the middle ($x=100$, blue). Note, since they almost exactly match we do not see the first channel as well in this plot. In the right subfigure (b) we show the absolute error of the simulated solution compared to the analytical solution as describe by \ref{eq:poiseuilleEq}.}
  \label{fig:Poiseuille_vectors}
  \end{center}
\end{figure}

In fig. \ref{fig:Poiseuille_vectors}(c) we plot the pressure along the centerline (i.e. $y=30$). We can observe that the pressure drops linearly from the inlet to the outlet. Note that there is a slight shift of the straight line. This is probably due to the fact the the boundary condition of the Poiseuille flow do not preserve mass. Also note that this is just a slight shift, thus it does not effect the velocity profile at the end as much.

Fig. \ref{fig:Poiseuille_evolution} shows the evolution of the Poiseuile Flow. We can observe that the parabolic velocity profile gets faster in every time step as well as more swaged. After around $5000$ time steps the velocity profile almost exactly matches the analytical solution given by eq. \ref{eq:poiseuilleEq}. After that there are only incremental steps towards the analytical solution.
\begin{figure}
  \begin{center}
	\scalebox{0.7}{\input{../figures/poiseuille_flow/vel_vectors_evolution.pgf}}
   \caption{Poiseuille flow evolution over $10000$ time steps. For sake of better visibility, we only use just $25\%$ of the time steps used in fig. \ref{fig:Poiseuille_vectors}. In the end there are just incremental steps towards the equilibrium that would not be visible in this plot due to its size. Note that the axis correspond to the same axis in fig. \ref{fig:Poiseuille_vectors}(a). However, we have left them out here for the sake of clarity.}
  \label{fig:Poiseuille_evolution}
  \end{center}
\end{figure}
\section{Von K\'{a}rm\'{a}n's vortex street}\label{sec-karman}
The von K\'{a}rm\'{a}n's vortex street is a well known phenomenon in fluid dynamics. It describes a process in which counter-rotating vortices are formed behind a body being flowed around by some fluid.
\begin{figure}
  \begin{center}
  \subfigure[Step 0]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/0.png}}
    \subfigure[Step 2000]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/2000.png}}
        \subfigure[Step 6400]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/6400.png}}
            \subfigure[Step 13500]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/13500.png}}
            \subfigure[Step 20000]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/20000.png}}
            \subfigure[Step 30000]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/30000.png}}
                        \subfigure[Step 40000]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/40000.png}}
            \subfigure[Step 45000]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/46500.png}}
            \subfigure[Step 30000]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/46500.png}}            
             \subfigure[Step 48000]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/48000.png}}
            \subfigure[Step 50000]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/50000.png}}
            \subfigure[Step 54000]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/54000.png}}
                         \subfigure[Step 56000]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/56000.png}}
            \subfigure[Step 60000]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/60000.png}}
            \subfigure[Step 70000]{\includegraphics[width=0.3\textwidth]{../figures/von_karman_vortex_shedding/all_png_parallel/70000.png}}
            
   \caption{Qualitative results of von K\'{a}rm\'{a}n's vortex street using the parallel implemenation. Qualitative results are chosen arbitrarily. In the aforementioned repository is a gif file showing a more continuous evolution.}
  \label{fig:vonKarmanQualitative}
  \end{center}
\end{figure}

\section{Scaling tests}\label{sec-scaling}
One key advantage of the parallelization is its likely speedup. However, according to Amdahl's law \cite{Amdahl.1967} the speedup $S$ is described as follows
\begin{equation}
S=\frac{p}{f_{p}+f_{s}p},
\end{equation}
where p is the number of available processors or cores and fractions $f_{s}$ and $f_{p}$ which specify whether a certain part of the code has to executed on a single core or can be executed on multiple cores, respectively. Note that $f_{s}+f_{p}=1$. Note that as the number of processors increases, the speedup is more influenced by the serial part of the code. One could quickly argue that we just have to avoid the serial part in some cases. However, in most cases we have to communicate between processes or synchronize processes. In our parallel \ac{lbm} implementation the serial part corresponds to the communication step right before the streaming step.
\chapter{Conclusions}\label{ch-conclusion}
In this report we described the \ac{lbm} and its implementation in Python. We also show several applications of the \ac{lbm} to specific problems and compare the simulation results to analytical solutions if possible. We extend the serial implementation by using spatial domain decomposition to reduce computational costs by means of parallelization.

In chapter \ref{ch-method} we describe the theoretical foundations of the \ac{lbm}. Starting from the continuous \ac{bte} we show how to discretize the \ac{bte} to obtain the \ac{lbm}.

In chapter \ref{ch-implementation} we discussed the implementation and more specifically the parallelization of the \ac{lbm}. We demonstrate by using the high-level language Python that implementations of the discrete equations are straightforward. We show how to reduce the computational burden by decompose the domain spatially into subdomains. Note, that the collision step is embarrassingly parallel. For the streaming step we use ghost cells to communicate adjacent lattice nodes to neighboring process. For the boundary conditions we transform global indices into local indicies in order to apply boundary conditions on the correct boundary nodes.

In chapter \ref{ch-results} we demonstrate in several applications the correctness of our \ac{lbm} implementation by comparing the simulation results to analytical solutions from the literature. In addition, we demonstrate the speedup that we can obtain from the spatial domain decomposition parallelization strategy of the \ac{lbm} for the von K\'{a}rm\'{a}n's vortex street.

\newpage

\bibliographystyle{unsrt}
\bibliography{biblio}

\end{document}
